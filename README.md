# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘       ğŸš€ Setup & Run llama.cpp with Nous-Hermes          â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€â”€â”€â”€â”€ 1. Install Dependencies (Choose your OS) â”€â”€â”€â”€â”€â”€

# ğŸ‘‰ Arch Linux:
sudo pacman -Syu --needed base-devel cmake git python-pip

# ğŸ‘‰ Ubuntu / Debian:
sudo apt update && sudo apt install -y build-essential cmake git python3-pip
pip install huggingface_hub


# â”€â”€â”€â”€â”€â”€ 2. Clone the Project Repository â”€â”€â”€â”€â”€â”€

git clone https://github.com/menashe12346/cyber_ai.git


# â”€â”€â”€â”€â”€â”€ 3. Download the Model (.gguf format) â”€â”€â”€â”€â”€â”€

# Optional: Login to HuggingFace (if required)
huggingface-cli login  # â† Paste your token when asked

# Download model file into current directory:
wget -P ./ \
https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_K_M.gguf


# â”€â”€â”€â”€â”€â”€ 4. Clone & Build llama.cpp â”€â”€â”€â”€â”€â”€

git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir build
cd build
cmake ..
cmake --build . --config Release -j$(nproc)


# â”€â”€â”€â”€â”€â”€ 5. Run the Model with a Prompt â”€â”€â”€â”€â”€â”€

# Run with basic test input (adjust path if needed):
./bin/llama-run ../Nous-Hermes-2-Mistral-7B-DPO.Q4_K_M.gguf "Hello! What is 2 + 2?"
