# âš”ï¸ Cyber-AI Offensive Framework  

Welcome to a fully autonomous offensive AI system for cyber attacks, driven by reinforcement learning and language models.  

---

## ğŸ”§ SETUP INSTRUCTIONS (ARCH LINUX)

### ğŸ“Œ Requirements:
- ğŸ Python 3.9+
- ğŸ§± C++17 compiler (e.g., `g++`)
- â— Disk space: ~6GB minimum, 10GB+ recommended
- ğŸ§  RAM: 8GB minimum, 16GB+ recommended for larger context

---

## ğŸš€ STEP 1: Install All Dependencies

### ğŸ‘‰ Arch Linux:
```bash
sudo pacman -Syu --needed python python-pip git base-devel cmake nmap metasploit wget curl unzip gcc make
sudo pacman -S virtualbox docker docker-compose  metasploit
pip install pexpect huggingface_hub torch numpy requests beautifulsoup4 pandas
yay -S zenmap armitage
```

### (Optional) Login to HuggingFace if the model is gated:
```bash
huggingface-cli login  # â† paste your token
```

---

## ğŸ’¾ STEP 2: Download the Model (GGUF Format)

> Download Nous-Hermes-2-Mistral-7B-DPO in quantized `.gguf` format (Q4_K_M recommended):

```bash
wget -P {your_project_directory}/code/models/nous-hermes/ \
https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_K_M.gguf
```

---

## ğŸ› ï¸ STEP 3: Clone & Build llama.cpp

```bash
cd {your_project_directory}/code/models/
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir build
cd build
cmake ..
cmake --build . --config Release -j$(nproc)
```

---

## ğŸ§  STEP 5: Run the Full Autonomous Framework

Once the model and dependencies are ready, launch the full AI system:

```bash
python main.py
```

The agent will:
- Initialize datasets (CVE, Exploits, OS, etc.)
- Encode the environment state
- Use the LLM to reason about reconnaissance and attacks
- Select and execute actions using reinforcement learning

---

## ğŸ“‚ Directory Structure

```text
project/
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ agents/               # Recon, Vuln, Exploit agents
â”‚   â”œâ”€â”€ blackboard/           # Shared knowledge base
â”‚   â”œâ”€â”€ encoders/             # State/Action encoders
â”‚   â”œâ”€â”€ models/               # Policy model, LLM wrapper
â”‚   â”œâ”€â”€ orchestrator/         # Scenario manager
â”‚   â””â”€â”€ utils/                # Helpers and tools
â”œâ”€â”€ datasets/                 # CVE, exploit, OS datasets
â”œâ”€â”€ models/                   # Saved LLM and policy model
â””â”€â”€ main.py                   # Entry point
```

---

## ğŸ§  What Does This Framework Do?

This is a **fully autonomous offensive AI system** that performs reconnaissance, vulnerability analysis, and exploitation â€” all without human input.

The system operates in discrete decision steps using **Reinforcement Learning (DQN)**, **LLMs** for reasoning, and a shared state representation for coordination.

---

## ğŸ§© Key Components

### ğŸ•µï¸ Reconnaissance Agent (`agents/recon_agent.py`)
- Selects the next command using DQN policy based on the current state
- Executes network and system-level recon commands (e.g., `nmap`, `whois`, `curl`, `gobuster`)
- Uses an LLM to interpret and extract structured information from unstructured command output
- Updates the shared JSON state

---

### ğŸ’£ Exploit Agent (`agents/exploit_agent.py`)
- Generates CPEs and maps them to top CVEs (via NVD)
- Uses DQN to choose from 2,500+ Metasploit exploits
- Executes exploits and parses results
- Calculates reward based on exploit success/failure and feeds it back to the model

---

### ğŸ“š Shared Knowledge (Blackboard Pattern) (`blackboard/`)
- Maintains a global, continuously updated JSON state
- Used by all agents to synchronize and reason
- Encodes: discovered services, ports, OS, vulnerabilities, and actions history

---

### ğŸ“¦ Encoders (`encoders/`)
- Transforms raw system/agent outputs into structured state vectors for RL
- Converts chosen actions into executable system commands

---

### ğŸ§  LLM Wrapper (`models/llm_wrapper.py`)
- Runs an instruction-following LLM (e.g., Nous-Hermes) using `llama.cpp`
- Parses recon output (HTML, terminal, JSON) and extracts entities like services, versions, CVEs, etc.

---

### ğŸ§ª Orchestrator (`orchestrator/`)
- Coordinates training episodes or attack scenarios
- Resets environment state, switches targets, logs metrics

---

### ğŸ§° Utils (`utils/`)
- Logging, metrics, graphing tools
- Parsing helpers and formatters

---

## ğŸ” Reinforcement Learning Loop

At each step:

1. Recon Agent observes state and chooses recon command  
2. LLM parses output â†’ shared state is updated  
3. Exploit Agent evaluates updated state and chooses exploit  
4. Reward is computed â†’ both agents' policies are updated  
5. Loop continues until goal is achieved or time expires

---

## ğŸ“¸ Live Execution Snapshots

Below are two screenshots demonstrating real-time execution of the AI offensive agent:

<p align="center">
  <img src="code/screenshots/screenshot_1750067604.png" alt="Recon Agent in Action" width="600"/>
</p>

<p align="center">
  <img src="code/screenshots/screenshot_1750067640.png" alt="Exploit Agent Success Example" width="600"/>
</p>

Each frame captures a different phase:
- ğŸ•µï¸â€â™‚ï¸ Recon Agent performing service enumeration and parsing output with the LLM
- ğŸ’¥ Exploit Agent launching a Metasploit module and receiving shell access

---

## ğŸ¯ Project Goals

- Demonstrate an autonomous AI agent capable of offensive cyber operations
- Combine symbolic reasoning (LLMs) with decision-making (RL)
- Enable interpretable, adaptive, and scalable attack workflows
