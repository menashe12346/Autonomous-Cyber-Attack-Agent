stage: sft
finetuning_type: lora
template: alpaca
model_name_or_path: /mnt/linux-data/project/code/models/hf_models/mistral
output_dir: /mnt/linux-data/project/code/models/finetuned_models/nous_hermes_parser_lora

dataset:
  path: /mnt/linux-data/project/code/datasets/parser_examples.jsonl
  type: alpaca

lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
target_modules: ["q_proj", "v_proj"]

per_device_train_batch_size: 1
gradient_accumulation_steps: 4
num_train_epochs: 5
learning_rate: 2e-5
logging_steps: 10
save_strategy: "epoch"
save_total_limit: 1
fp16: true
