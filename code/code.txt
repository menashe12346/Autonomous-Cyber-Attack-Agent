<</mnt/linux-data/project/code/get_code.py>>
import os, tiktoken

def count_tokens(text):
    enc = tiktoken.get_encoding("cl100k_base")  # ××ª××™× ×œ-GPT-4o
    return len(enc.encode(text, disallowed_special=()))

def print_all_python_files(start_dir, output_file):
    skip_dirs = {
        "llama.cpp",
        "nous-hermes",
        "__pycache__",
        "models--google--flan-t5-xl"
    }

    output = ''
    for root, dirs, files in os.walk(start_dir):
        dirs[:] = [d for d in dirs if d not in skip_dirs]

        for file in files:
            if file.endswith(".py"):
                path = os.path.join(root, file)
                try:
                    with open(path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        snippet = f"<<{path}>>\n{content}\n"
                        output += snippet
                        print(snippet)
                except Exception as e:
                    print(f"[ERROR] Failed to read {path}: {e}")

    with open(output_file, 'w', encoding='utf-8') as out_f:
        out_f.write(output)

    print(f"\n[Total Tokens (GPT-4o): {count_tokens(output)}]")
    print(f"[Saved to: {output_file}]")

# ×”×¤×¢×œ×ª ×”×¡×§×¨×™×¤×˜
print_all_python_files(
    "/mnt/linux-data/project/code",
    "/mnt/linux-data/project/code/code.txt"
)

<</mnt/linux-data/project/code/main.py>>
import torch
from blackboard.blackboard import initialize_blackboard
from blackboard.api import BlackboardAPI
from replay_buffer.Prioritized_Replay_Buffer import PrioritizedReplayBuffer
from agents.agent_manager import AgentManager
from orchestrator.scenario_orchestrator import ScenarioOrchestrator
from models.policy_model import PolicyModel
from models.trainer import RLModelTrainer
from encoders.state_encoder import StateEncoder
from encoders.action_encoder import ActionEncoder
from tools.action_space import get_commands_for_agent
from agents.recon_agent import ReconAgent
from models.LoadModel import LoadModel

def main():
    NUM_EPISODES = 30
    MAX_STEPS_PER_EPISODE = 5
    LLAMA_RUN = "/mnt/linux-data/project/code/models/llama.cpp/build/bin/llama-run" # change to your path
    MODEL_PATH = "file:///mnt/linux-data/project/code/models/nous-hermes/Nous-Hermes-2-Mistral-7B-DPO.Q4_K_M.gguf" # change to your path
    model = LoadModel(LLAMA_RUN, MODEL_PATH)
    TARGET_IP = "192.168.56.101"

    # ××ª×—×•×œ Replay Buffer ××©×•×ª×£ ×œ×›×œ ×”×¤×¨×§×™×
    replay_buffer = PrioritizedReplayBuffer(max_size=20000)

    # ××ª×—×•×œ Action Space ×§×‘×•×¢ ××¨××© (×œ×¤×™ IP ×§×‘×•×¢)
    action_space = get_commands_for_agent("recon", TARGET_IP)
    action_encoder = ActionEncoder(action_space)
    state_encoder = StateEncoder(action_space=action_space)

    # ××ª×—×•×œ Policy Model
    state_size = 128
    action_size = len(action_space)
    policy_model = PolicyModel(state_size=state_size, action_size=action_size)

    # ××¢×‘×¨ ×œÖ¾GPU ×× ×–××™×Ÿ
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    policy_model.to(device)

    # ××ª×—×•×œ ××××Ÿ RL
    trainer = RLModelTrainer(policy_model, replay_buffer, device=device, learning_rate=1e-3, gamma=0.99)

    command_cache = {}

    all_actions = []  # ×¨×©×™××ª ×›×œ ×”×¤×¢×•×œ×•×ª ×©×œ ×›×œ ×”××¤×™×–×•×“×•×ª

    # ×”×¨×¦×ª ××¡×¤×¨ ×¤×¨×§×™× (×¡×™××•×œ×¦×™×•×ª)
    for episode in range(NUM_EPISODES):
        print(f"\n========== EPISODE {episode + 1} ==========")

        # ××ª×—×•×œ ××—×“×© ×©×œ ×”Ö¾Blackboard ×œ×¤×¨×§ × ×¤×¨×“
        blackboard_dict = initialize_blackboard()
        blackboard_dict["target"]["ip"] = TARGET_IP
        bb_api = BlackboardAPI(blackboard_dict)

        # ×™×¦×™×¨×ª ×”×¡×•×›×Ÿ ××—×“×© (state ×¤× ×™××™ ×—×“×© ×‘×›×œ ×¤×¢×)
        recon_agent = ReconAgent(
            blackboard_api=bb_api,
            policy_model=policy_model,
            replay_buffer=replay_buffer,
            state_encoder=state_encoder,
            action_encoder=action_encoder,
            command_cache = command_cache,
            model = model
        )

        agents = [recon_agent]
        agent_manager = AgentManager(bb_api)
        agent_manager.register_agents(agents)

        orchestrator = ScenarioOrchestrator(
            blackboard=bb_api,
            agent_manager=agent_manager,
            max_steps=MAX_STEPS_PER_EPISODE,
            scenario_name=f"AttackEpisode_{episode + 1}",
            target=TARGET_IP
        )

        orchestrator.run_scenario_loop()

        all_actions.append({
            "episode": episode + 1,
            "actions": recon_agent.actions_history.copy()
        })

        # ×©×œ×‘ ××™××•×Ÿ ×§×¦×¨ ××—×¨×™ ×›×œ ×¤×¨×§ (××¤×©×¨ ×’× ×›×œ ×›××” ×¤×¨×§×™×)
        for _ in range(10):  # ×œ××©×œ 10 ××™×˜×¨×¦×™×•×ª ××™××•×Ÿ
            loss = trainer.train_batch(batch_size=32)
            if loss is not None:
                print(f"[Episode {episode + 1}] Training loss: {loss:.4f}")
    
    print("\n========== SUMMARY OF ALL EPISODES ==========")
    for episode_info in all_actions:
        print(f"Episode {episode_info['episode']}: {episode_info['actions']}")


    # ×©××™×¨×” ×¡×•×¤×™×ª ×©×œ ×”××•×“×œ
    trainer.save_model("models/saved_models/policy_model.pth")
    print("âœ… Final trained model saved.")

if __name__ == "__main__":
    main()

<</mnt/linux-data/project/code/replay_buffer/replay_buffer.py>>
###### Not using it now ######

import random

class ReplayBuffer:
    def __init__(self, max_size=10000):
        self.buffer = []
        self.max_size = max_size

    def add_experience(self, state, action, reward, next_state):
        """
        ××•×¡×™×£ × ×™×¡×™×•×Ÿ ×—×“×© ×œ×××’×¨ ×”×–×™×›×¨×•×Ÿ.
        """
        experience = {
            "state": state,
            "action": action,
            "reward": reward,
            "next_state": next_state
        }
        if len(self.buffer) >= self.max_size:
            self.buffer.pop(0)  # FIFO â€“ ××•×—×§ ××ª ×”×¨××©×•×Ÿ
        self.buffer.append(experience)

    def sample_batch(self, batch_size):
        """
        ××—×–×™×¨ ×ª×ªÖ¾×§×‘×•×¦×” ××§×¨××™×ª ××”×××’×¨ ×œ×œ××™×“×”.
        """
        return random.sample(self.buffer, min(batch_size, len(self.buffer)))

    def get_recent(self, n=5):
        """
        ××—×–×™×¨ ××ª n ×”× ×™×¡×™×•× ×•×ª ×”××—×¨×•× ×™×.
        """
        return self.buffer[-n:]

    def clear(self):
        """
        ×××¤×¡ ××ª ×”×–×™×›×¨×•×Ÿ ×œ×—×œ×•×˜×™×Ÿ.
        """
        self.buffer.clear()

    def size(self):
        return len(self.buffer)

<</mnt/linux-data/project/code/replay_buffer/Prioritized_Replay_Buffer.py>>
import random
import numpy as np
import torch

class PrioritizedReplayBuffer:
    def __init__(self, max_size=100000, alpha=0.6, beta=0.4):
        self.max_size = max_size
        self.alpha = alpha  # alpha ×§×•×‘×¢ ××ª ×¢×•×¦××ª ×”-priorities
        self.beta = beta    # beta ×§×•×‘×¢ ×¢×“ ×›××” × ×¢×“×™×£ ×—×•×•×™×•×ª ×¢× priorites ×’×‘×•×”×™×
        self.buffer = []
        self.priorities = []

    def add_experience(self, state, action, reward, next_state, done):
        """
        ××•×¡×™×£ × ×™×¡×™×•×Ÿ ×—×“×© ×œ×××’×¨ ×¢× priority.
        """
        priority = max(self.priorities) if self.buffer else 1.0  # ××ª×—×•×œ priority
        experience = {
            "state": state,
            "action": action,
            "reward": reward,
            "next_state": next_state,
            "done": done
        }

        if len(self.buffer) >= self.max_size:
            self.buffer.pop(0)
            self.priorities.pop(0)

        self.buffer.append(experience)
        self.priorities.append(priority)

    def sample_batch(self, batch_size):
        """
        ××—×–×™×¨ ×“×’×™××” ××§×¨××™×ª ×©×œ ×—×•×•×™×•×ª ××”×××’×¨ ×œ×¤×™ ×¢×“×™×¤×•×ª.
        """
        priorities = np.array(self.priorities)
        scaled_priorities = priorities ** self.alpha
        probabilities = scaled_priorities / np.sum(scaled_priorities)

        indices = np.random.choice(len(self.buffer), size=batch_size, p=probabilities)
        batch = [self.buffer[i] for i in indices]
        weights = (len(self.buffer) * probabilities[indices]) ** -self.beta
        weights /= weights.max()

        states = torch.stack([ex['state'] for ex in batch])
        actions = torch.tensor([ex['action'] for ex in batch])
        rewards = torch.tensor([ex['reward'] for ex in batch])
        next_states = torch.stack([ex['next_state'] for ex in batch])
        dones = torch.tensor([ex['done'] for ex in batch])

        return states, actions, rewards, next_states, dones, weights, indices

    def update_priorities(self, indices, priorities):
        """
        ××¢×“×›×Ÿ ××ª ×”-priorities ×©×œ ×—×•×•×™×•×ª ××¡×•×™××•×ª ×œ×¤×™ ×—×•×•×™×•×ª ×××•×—×¨×•×ª.
        """
        for idx, priority in zip(indices, priorities):
            self.priorities[idx] = priority

    def size(self):
        return len(self.buffer)

    def clear(self):
        self.buffer.clear()
        self.priorities.clear()

<</mnt/linux-data/project/code/agents/base_agent.py>>
from abc import ABC, abstractmethod
import random
import subprocess
from models.prompts import PROMPT_1, PROMPT_2, clean_output_prompt, PROMPT_FOR_A_PROMPT
import json
import re
from utils.utils import remove_comments_and_empty_lines, extract_json_block
from Cache.llm_cache import LLMCache

class BaseAgent(ABC):
    def __init__(self, name, action_space, blackboard_api, replay_buffer, policy_model, state_encoder, action_encoder, command_cache, model, epsilon=0.1):
        self.name = name
        self.action_space = action_space
        self.blackboard_api = blackboard_api
        self.replay_buffer = replay_buffer
        self.policy_model = policy_model
        self.state_encoder = state_encoder
        self.action_encoder = action_encoder
        self.last_state = None
        self.last_action = None
        self.epsilon = epsilon
        self.actions_history = [] 
        self.command_cache = command_cache  # ×¤×§×•×“×•×ª ×©×”×•×¨×¦×• ×•×”×ª×•×¦××” × ×©××¨×”
        self.model = model
        self.llm_cache = LLMCache()

    @abstractmethod
    def should_run(self) -> bool:
        """
        ×‘×•×“×§×ª ×× ×”×¡×•×›×Ÿ ×¦×¨×™×š ×œ×¤×¢×•×œ ×›×¢×ª ×œ×¤×™ ××¦×‘ ×”××¢×¨×›×ª, × ×™×¡×™×•× ×•×ª ×§×•×“××™×, ×•×”×’×“×¨×•×ª.
        """
        pass

    def run(self):
        """
        ×”×œ×•×œ××” ×”×¨××©×™×ª ×©×œ ×”×¡×•×›×Ÿ â€“ ××™×™×¦×’×ª × ×™×¡×™×•×Ÿ ×¤×¢×•×œ×” ×•×œ××™×“×” ××—×ª.
        """
        raw_state = self.get_state_raw()  # ×©×œ×™×¤×ª ××¦×‘ ×’×•×œ××™
        raw_state_with_history = dict(raw_state)
        raw_state_with_history["actions_history"] = self.actions_history.copy()
        state = self.state_encoder.encode(raw_state_with_history, self.actions_history)
        self.last_state = state

        print(f"last state: {raw_state_with_history}")

        action = self.choose_action(state)
        self.last_action = action

        self.actions_history.append(action)  # ×”×•×¡×¤×ª ×¤×¢×•×œ×” ×œ×”×™×¡×˜×•×¨×™×”

        print(f"\n[+] Agent: {self.name}")
        print(f"    Current state: {str(state)[:8]}...")  # ××§×¦×¨ ××ª ×”×”×“×¤×¡×”
        print(f"    Chosen action: {action}")

        result = remove_comments_and_empty_lines(self.perform_action(action))
        print("\033[1;32m" + str(result) + "\033[0m")

        # ×¨×§ ×× ×”×¤×œ×˜ ×›×•×œ×œ ×™×•×ª×¨ ×Ö¾300 ××™×œ×™× â€“ ×œ×‘×¦×¢ × ×™×§×•×™
        if len(result.split()) > 300:
            try:
                cleaned_output = self.clean_output(clean_output_prompt(result))
            except Exception as e:
                print(f"[!] Failed to clean output: {e}")
                cleaned_output = result
        else:
            cleaned_output = result
        print(f"\033[94mcleaned_output - {cleaned_output}\033[0m")

        parsed_info = self.parse_output(cleaned_output)
        print(f"parsed_info - {parsed_info}")
        self.blackboard_api.overwrite_blackboard(parsed_info)

        raw_next_state = self.get_state_raw()
        raw_next_state_with_history = dict(raw_next_state)
        raw_next_state_with_history["actions_history"] = self.actions_history.copy()
        next_state = self.state_encoder.encode(raw_next_state_with_history, self.actions_history)
        reward = self.get_reward(state, action, next_state)

        print(f"new state: {dict(self.state_encoder.decode(next_state))}")

        # ×‘× ×™×™×ª ×§×œ×˜ ×œ××•×“×œ ×œ×¦×•×¨×š ×¢×“×›×•×Ÿ ×•×œ×•×’
        experience = {
            "state": state,
            "action": self.action_space.index(action),
            "reward": reward,
            "next_state": next_state
        }

        # ×¢×“×›×•×Ÿ ××•×“×œ ×•×—×™×©×•×‘ q_pred + loss
        q_pred, loss = self.policy_model.update(experience)

        print(f"    Predicted Q-value: {q_pred:.4f}")
        print(f"    Actual reward:     {reward:.4f}")
        print(f"    Loss:              {loss:.6f}")

        # ×”×•×¡×¤×ª ×—×•×•×™×” ×œ×××’×¨
        self.replay_buffer.add_experience(state, self.action_space.index(action), reward, next_state, False)

        self.blackboard_api.append_action_log({
            "agent": self.name,
            "action": action,
            "result": result,
        })


    def get_state_raw(self):
        """
        ××—×œ×¥ ××ª ××¦×‘ ×”Ö¾blackboard_api ×›×¤×™ ×©×”×•×, ×œ×¦×•×¨×š ×§×™×“×•×“ ×¢× ×”×™×¡×˜×•×¨×™×”.
        """
        return self.blackboard_api.get_state_for_agent(self.name)

    def get_state(self):
        """
        ××—×œ×¥ ××ª ××¦×‘ ×”-blackboard_api ×•××§×•×“×“ ××•×ª×• ×œ×•×§×˜×•×¨ ×§×œ×˜ ×œ××•×“×œ.
        """
        return self.state_encoder.encode(self.get_state_raw(), self.actions_history)

    def choose_action(self, state_vector):
        """
        ×‘×•×—×¨×ª ×¤×¢×•×œ×” ×œ×¤×™ ××¡×˜×¨×˜×’×™×™×ª Îµ-greedy:
        - ×‘×”×¡×ª×‘×¨×•×ª Îµ: ×¤×¢×•×œ×” ××§×¨××™×ª (exploration)
        - ×‘×”×¡×ª×‘×¨×•×ª 1-Îµ: ×”×¤×¢×•×œ×” ×”×›×™ ×˜×•×‘×” ×œ×¤×™ policy_model (exploitation)
        """
        if random.random() < self.epsilon:
            # ×¤×¢×•×œ×” ××§×¨××™×ª (×—×§×™×¨×”)
            action_index = random.randint(0, len(self.action_space) - 1)
            self.decay_epsilon()
        else:
            # ×”×¤×¢×•×œ×” ×”×›×™ ×˜×•×‘×” ×œ×¤×™ ×”××•×“×œ
            action_index = self.policy_model.predict_best_action(state_vector)

        return self.action_space[action_index]
    
    def decay_epsilon(self, decay_rate=0.995, min_epsilon=0.01):
        self.epsilon = max(self.epsilon * decay_rate, min_epsilon)

    def perform_action(self, action: str) -> str:
        """
        ×‘×¨×™×¨×ª ××—×“×œ ×œ×”×¤×¢×œ×ª ×¤×¢×•×œ×” ×¢"×™ ×¤×§×•×“×ª ×˜×¨××™× ×œ ×¢×œ ×›×ª×•×‘×ª IP ×©×œ ×”×§×•×¨×‘×Ÿ.
        ××™×•×¢×“ ×œ×¡×•×›× ×™× ×©××©×ª××©×™× ×‘×¤×§×•×“×•×ª ××‘×•×¡×¡×•×ª IP.
        """

        ip = self.blackboard_api.blackboard.get("target", {}).get("ip", "127.0.0.1")
        command = action.format(ip=ip)

        if action in self.command_cache:
            print(f"[Cache] Returning cached result for action: {action}")
            return self.command_cache[action]

        try:
            output = subprocess.check_output(command.split(), timeout=10).decode()
        except Exception as e:
            self.blackboard_api.add_error(self.name, action, str(e))
            output = ""

        self.command_cache[action] = output

        return output

    def parse_output(self, command_output: str) -> dict:
        """
        ××¤×¢×™×œ ××ª ××•×“×•×œ ×”×¤×¢× ×•×— ×•××—×–×™×¨ ×ª×•×‘× ×•×ª ×œÖ¾blackboard_api.
        ×›×•×œ×œ ×©×™××•×© ×—×›× ×‘××˜××•×Ÿ ×œ×¤×™ ××¦×‘ ×•×¤×¢×•×œ×”.
        """

        # ××¦×‘ ×¢× ×”×™×¡×˜×•×¨×™×” (×œ×¦×•×¨×š ×™×™×—×•×“×™×•×ª ×‘×–×™×”×•×™ state-action)
        raw_state = self.get_state_raw()
        raw_state["actions_history"] = self.actions_history.copy()

        # × ×™×¡×™×•×Ÿ ×œ×©×œ×•×£ ××”××˜××•×Ÿ
        cached = self.llm_cache.get(raw_state, self.last_action)
        if cached:
            print("\033[93m[CACHE] Using cached LLM result.\033[0m")
            return cached

        # ×”×¤×¢×œ×ª prompt ××•×ª×× ×œ×¡×•×’ ×”×¤×œ×˜
        prompt_for_cleaning = PROMPT_FOR_A_PROMPT(command_output)
        inner_prompt = self.model.run_prompt(prompt_for_cleaning)
        final_prompt = PROMPT_2(command_output, inner_prompt)

        # ×©×œ×™×—×ª ×›×œ ×”×¤×¨×•××¤×˜×™× ×œ××•×“×œ
        full_response = self.model.run_prompts([PROMPT_1, final_prompt])
        print(f"full_response - {full_response}")

        # ×—×™×œ×•×¥ JSON ×ª×§× ×™ ××”×¤×œ×˜
        parsed = extract_json_block(full_response)

        # ×©××™×¨×” ×œ××˜××•×Ÿ
        if parsed:
            self.llm_cache.set(raw_state, self.last_action, parsed)

        return parsed

    def clean_output(self, command_output: str) -> dict:
        return self.model.run_prompt(clean_output_prompt(command_output))

    @abstractmethod
    def get_reward(self, prev_state, action, next_state) -> float:
        """
        ××—×©×‘ ××ª ×¢×¨×š ×”×ª×’××•×œ ×©×”×¤×¢×•×œ×” ×”×©×™×’×”, ×œ×¤×™ ×”×©×™× ×•×™ ×‘××¦×‘.
        """
        # (×™×™×©×•× ×—×™×¦×•× ×™ â€“ reward_calculator)
        raise NotImplementedError

    def update_policy(self, state, action, reward, next_state):
        """
        ×©×•×œ×— ××ª ×”× ×ª×•× ×™× ×œ-Replay Buffer ×•×”××•×“×œ ××ª×¢×“×›×Ÿ ×‘×”×ª××.
        """
        self.policy_model.update({
            "state": state,
            "action": self.action_space.index(action),
            "reward": reward,
            "next_state": next_state
        })

<</mnt/linux-data/project/code/agents/recon_agent.py>>
from agents.base_agent import BaseAgent
from tools.action_space import get_commands_for_agent
import hashlib
import json

class ReconAgent(BaseAgent):
    def __init__(self, blackboard_api, policy_model, replay_buffer, state_encoder, action_encoder, command_cache, model):
        ip = blackboard_api.blackboard["target"]["ip"]  # ×©×œ×™×¤×ª IP ××”××˜×¨×”
        super().__init__(
            name="ReconAgent",
            blackboard_api=blackboard_api,
            action_space=get_commands_for_agent("recon", ip),
            policy_model=policy_model,
            replay_buffer=replay_buffer,
            state_encoder=state_encoder,
            action_encoder=action_encoder,
            command_cache=command_cache,
            model=model
        )

    def should_run(self):
        """
        ××—×œ×™×˜ ×× ×”×¡×•×›×Ÿ ×¦×¨×™×š ×œ×¤×¢×•×œ ×œ×¤×™ ×›×œ×œ ×”××™×“×¢ ×©×‘Ö¾Blackboard,
        ×›×•×œ×œ ×©×™×¨×•×ª×™×, ×¤×•×¨×˜×™×, ×©×’×™××•×ª, shell ×¤×ª×•×—, ×–×™×”×•×™ ×¢×´×™ ×”×’× ×•×ª ×•×¢×•×“.
        """
        bb = self.blackboard_api.blackboard
        target = bb.get("target", {})
        runtime = bb.get("runtime_behavior", {})
        actions_log = bb.get("actions_log", [])
        errors = bb.get("errors", [])
        impact = bb.get("attack_impact", {})

        services = target.get("services", [])
        open_ports = target.get("open_ports", [])

        # 1. ××™×Ÿ ×©×™×¨×•×ª×™× ×•××™×Ÿ ×¤×•×¨×˜×™× ×›×œ×œ => ×—×•×‘×” ×œ×”×¨×™×¥
        if not services and not open_ports:
            return True

        # 2. ×× ×™×© ×¤×—×•×ª ×Ö¾2 ×©×™×¨×•×ª×™× ×•×¤×•×¨×˜×™× => ×›× ×¨××” ×”×¡×¨×™×§×” ×—×œ×§×™×ª
        if len(services) < 2 and len(open_ports) < 2:
            return True

        # 3. ×× ×”×™×™×ª×” ×©×’×™××” ×‘×¡×¨×™×§×” ×§×•×“××ª ××”×¡×•×›×Ÿ ×”×–×”
        for err in errors:
            if err.get("agent") == self.name:
                return True

        # 4. ×× ×¢×‘×¨×• ×™×•×ª×¨ ×Ö¾5 ×“×§×•×ª ×××– ×”×¤×¢× ×”××—×¨×•× ×” ×©×”×¡×•×›×Ÿ ×¤×¢×œ
        last_time = None
        for log in reversed(actions_log):
            if log.get("agent") == self.name:
                last_time = log.get("timestamp")
                break
        if last_time is None:
            return True  # ×”×¡×•×›×Ÿ ×˜×¨× ×¤×¢×œ

        import time
        if time.time() - last_time > 300:
            return True

        # 5. ×× ×›×‘×¨ ×™×© shell ×¤×ª×•×— â‡’ ××•×œ×™ ××™×•×ª×¨ ×œ×¡×¨×•×§
        shell = runtime.get("shell_opened", {})
        if shell.get("shell_type") and shell.get("shell_access_level"):
            return False

        # 6. ×× ×”×¡×•×›×Ÿ × ×—×©×£ â€“ ××•×œ×™ ×œ× × ×¨×¦×” ×œ×¡×¨×•×§ ×©×•×‘ ×›×¨×’×¢
        if impact.get("detected_by_defenses", False):
            return False

        # ×× ×›×œ×•× ×××œ×” ×œ× ×§×¨×” â€“ ×œ× ×¦×¨×™×š ×œ×”×¨×™×¥
        #return False
        return True # for debug

    def get_reward(self, prev_state, action, next_state) -> float:
        """
        ××—×©×‘ ×ª×’××•×œ ×œ×¤×™:
        - ×©×™× ×•×™ ×‘××™×“×¢ (×©×™×¨×•×ª×™×, ×¤×•×¨×˜×™×, shell)
        - ×©×™××•×© ×—×•×–×¨ ××• ×—×“×©× ×™ ×‘×¤×¢×•×œ×”
        - ×™×¢×™×œ×•×ª ×”×¤×¢×•×œ×”
        """
        reward = 0.0

        def _services_to_set(services):
            return set(
                (s.get("port", ""), s.get("protocol", ""), s.get("service", ""))
                for s in services if isinstance(s, dict)
            )

        try:
            # ×—×™×œ×•×¥ hash ××–×”×” ×œÖ¾encoded state
            prev_hash = hashlib.sha256(json.dumps(prev_state.tolist(), sort_keys=True).encode()).hexdigest()
            next_hash = hashlib.sha256(json.dumps(next_state.tolist(), sort_keys=True).encode()).hexdigest()

            prev_dict = self.state_encoder.encoded_to_state.get(prev_hash, {})
            next_dict = self.state_encoder.encoded_to_state.get(next_hash, {})

            actions_history = prev_dict.get("actions_history", [])

            # âœ´ï¸ ×—×–×¨×ª×™×•×ª
            if action in actions_history:
                reward -= 0.5  # ×¢× ×™×©×” ×¢×œ ×—×–×¨×ª×™×•×ª
            else:
                reward += 0.2  # ×ª×’××•×œ ×§×œ ×¢×œ ×—×§×™×¨×” ×—×“×©×”

            # âœ´ï¸ ×©×™×¨×•×ª×™× ×—×“×©×™×
            prev_services = _services_to_set(prev_dict.get("target", {}).get("services", []))
            next_services = _services_to_set(next_dict.get("target", {}).get("services", []))
            new_services = next_services - prev_services
            reward += 1.0 * len(new_services)

            # âœ´ï¸ ×¤×•×¨×˜×™× ×¤×ª×•×—×™× (×× ×§×™×™××™×)
            prev_ports = set(prev_dict.get("target", {}).get("open_ports", []))
            next_ports = set(next_dict.get("target", {}).get("open_ports", []))
            new_ports = next_ports - prev_ports
            reward += 0.5 * len(new_ports)

            # âœ´ï¸ ×¤×ª×™×—×ª shell ×—×“×©
            prev_shell = prev_dict.get("runtime_behavior", {}).get("shell_opened", {})
            next_shell = next_dict.get("runtime_behavior", {}).get("shell_opened", {})

            if not prev_shell.get("shell_type") and next_shell.get("shell_type"):
                reward += 5.0  # ×‘×•× ×•×¡ ×’×“×•×œ ×¢×œ shell

            # âœ´ï¸ ×©×“×¨×•×’ ×¨××ª ×’×™×©×”
            levels = {"": 0, "user": 1, "root": 2}
            prev_level = prev_shell.get("shell_access_level", "")
            next_level = next_shell.get("shell_access_level", "")
            if levels.get(next_level, 0) > levels.get(prev_level, 0):
                reward += 3.0

            # âœ´ï¸ ×¢× ×™×©×” ×× ××™×Ÿ ×©×™× ×•×™ ×•×”×¤×§×•×“×” ×—×–×¨×” ×¢×œ ×¢×¦××”
            if not new_services and not new_ports and not next_shell.get("shell_type"):
                if action in actions_history:
                    reward -= 1.0  # ×¢× ×™×©×” ×¢×œ ×‘×–×‘×•×– ×¤×¢×•×œ×”

            # ğŸ” DEBUG
            print(f"[Reward Debug] Action: {action}")
            print(f"[Reward Debug] New services: {len(new_services)}")
            print(f"[Reward Debug] New ports: {len(new_ports)}")
            print(f"[Reward Debug] Shell opened: {next_shell.get('shell_type')}")
            print(f"[Reward Debug] Total reward: {reward:.4f}")

            return reward

        except Exception as e:
            print(f"[!] Reward computation failed: {e}")
            return 0.0

<</mnt/linux-data/project/code/agents/agent_manager.py>>
class AgentManager:
    def __init__(self, blackboard_api):
        self.agents = []
        self.blackboard = blackboard_api
        self.current_index = 0
        self.execution_log = []
        self.actions_history = []  # ×¨×©×™××” ×©×ª×©××•×¨ ××ª ×›×œ ×”×¤×¢×•×œ×•×ª ×©×‘×•×¦×¢×• ×‘××¢×¨×›×ª

    def register_agents(self, agent_list):
        """
        ××§×‘×œ ×¨×©×™××ª ×¡×•×›× ×™× ×•××¨×™×¥ ×ª×”×œ×™×š ×¨×™×©×•×.
        """
        self.agents = agent_list
        self.current_index = 0
        self.execution_log.clear()

    def run_all(self):
        """
        ××¨×™×¥ ××ª ×›×œ ×”×¡×•×›× ×™× ×©Ö¾should_run ×©×œ×”× ××—×–×™×¨ True.
        """
        for agent in self.agents:
            if agent.should_run():
                agent.run()
                self.execution_log.append(agent.name)
                self.actions_history.append(agent.last_action)

    def run_step(self):
        """
        ××¨×™×¥ ××ª ×”×¡×•×›×Ÿ ×”×‘× ×‘×ª×•×¨ ×× ×”×•× ××•×›×Ÿ ×œ×¤×¢×•×œ×”.
        """
        if not self.agents:
            return

        agent = self.agents[self.current_index]
        self.current_index = (self.current_index + 1) % len(self.agents)

        if agent.should_run():
            agent.run()
            self.execution_log.append(agent.name)
            self.actions_history.append(agent.last_action)

    def has_pending_actions(self):
        """
        ×‘×•×“×§ ×× ×™×© ×¡×•×›× ×™× ×©×¢×“×™×™×Ÿ ×¢×©×•×™×™× ×œ×¤×¢×•×œ.
        """
        return any(agent.should_run() for agent in self.agents)

    def log_summary(self):
        """
        ××¦×™×’ ××ª ×¨×©×™××ª ×”×¡×•×›× ×™× ×©×¤×¢×œ×• ×¢×“ ×›×”.
        """
        print("Agents executed in this round:")
        for name in self.execution_log:
            print(f"âœ… {name}")
            print(self.actions_history)

<</mnt/linux-data/project/code/Cache/llm_cache.py>>
import hashlib
import json
import os
import pickle

class LLMCache:
    def __init__(self, cache_file="llm_cache.pkl"):
        self.cache_file = cache_file
        self.cache = self._load_cache()

    def _load_cache(self):
        if os.path.exists(self.cache_file):
            with open(self.cache_file, "rb") as f:
                return pickle.load(f)
        return {}

    def _save_cache(self):
        with open(self.cache_file, "wb") as f:
            pickle.dump(self.cache, f)

    def _hash(self, state, action):
        data = {
            "state": state,
            "action": action
        }
        state_action_str = json.dumps(data, sort_keys=True)
        return hashlib.sha256(state_action_str.encode()).hexdigest()

    def get(self, state, action):
        key = self._hash(state, action)
        return self.cache.get(key)

    def set(self, state, action, value):
        key = self._hash(state, action)
        self.cache[key] = value
        self._save_cache()
<</mnt/linux-data/project/code/tools/action_space.py>>
from typing import List

COMMAND_TEMPLATES = {
    "ping": [
        "ping -c 1 {ip}"
    ],
    "nmap_fast": [
        #"nmap -F {ip}"
    ],
    "nmap_services": [
        "nmap {ip}"
    ],
    "curl_headers": [
        "curl -I http://{ip}"
    ],
    "curl_index": [
        "curl http://{ip}/"
    ],
    "wget_index": [
        "wget http://{ip} -O -"
    ],
    "traceroute": [
        "traceroute {ip}"
    ],
    "web_tech": [
        "whatweb http://{ip}"
    ],
    "gobuster_scan": [
        "gobuster dir -u http://{ip} -w /mnt/linux-data/wordlists/SecLists/Discovery/Web-Content/common.txt"
    ]
}

TOOLS = list(COMMAND_TEMPLATES.keys())

def build_action_space(ip: str) -> List[str]:
    """
    ×‘×•× ×” ××ª ×›×œ ×”×¤×§×•×“×•×ª ×”××¤×©×¨×™×•×ª ×¢× ×›×ª×•×‘×ª ×”-IP ×©× ×™×ª× ×”.
    """
    actions = []
    for tool, templates in COMMAND_TEMPLATES.items():
        for cmd in templates:
            actions.append(cmd.format(ip=ip))
    return actions

def get_commands_for_agent(agent_type: str, ip: str) -> List[str]:
    """
    ××—×–×™×¨ ×¨×©×™××ª ×¤×§×•×“×•×ª ×‘×”×ª×× ×œ×¡×•×’ ×”×¡×•×›×Ÿ.
    
    ×›×¨×’×¢, ×›×œ ×¡×•×’ ×”×¡×•×›×Ÿ ××©×ª××© ×‘×¤×•× ×§×¦×™×” build_action_space,
    ××š ×‘×¢×ª×™×“ × ×™×ª×Ÿ ×œ×”×¨×—×™×‘ ×•×œ×¡× ×Ÿ ×¤×§×•×“×•×ª ×œ×¤×™ agent_type.
    """
    # ×“×•×’××”: ×× ×”×¡×•×›×Ÿ Recon, × ×©×ª××© ×‘×›×œ ×”×¤×§×•×“×•×ª ×”××”×™×¨×•×ª
    if agent_type.lower() == "recon":
        return build_action_space(ip)
    # ××¤×©×¨ ×œ×”×•×¡×™×£ ×ª× ××™× × ×•×¡×¤×™× ×¢×‘×•×¨ ×¡×•×’×™ ×¡×•×›× ×™× ××—×¨×™× (access, exec ×•×›×•')
    return build_action_space(ip)

if __name__ == "__main__":
    target_ip = "192.168.56.101"
    cmds = get_commands_for_agent("recon", target_ip)
    for cmd in cmds:
        print(cmd)

<</mnt/linux-data/project/code/orchestrator/scenario_orchestrator.py>>
class ScenarioOrchestrator:
    def __init__(self, blackboard, agent_manager, target, max_steps=20, scenario_name="DefaultScenario", stop_conditions=None):
        self.blackboard = blackboard
        self.agent_manager = agent_manager
        self.max_steps = max_steps
        self.current_step = 0
        self.scenario_name = scenario_name
        self.stop_conditions = stop_conditions or []
        self.active = False
        self.target=target

    def start(self):
        """
        ×××ª×—×œ ××ª ×”×ª×¨×—×™×©: ××™×¤×•×¡ ××“×“×™×, ×œ×•×’×™×, ××™×ª×—×•×œ Blackboard ×•Ö¾AgentManager.
        """
        self.current_step = 0
        self.active = True
        self.blackboard.blackboard["target"] = {
            "ip": self.target,
            "os": "Unknown",
            "services": [
                {"port": "", "protocol": "", "service": ""},
                {"port": "", "protocol": "", "service": ""},
                {"port": "", "protocol": "", "service": ""}
            ]
        }
        self.blackboard.blackboard["web_directories_status"] = {
            "404": { "": "" },
            "200": { "": "" },
            "403": { "": "" },
            "401": { "": "" },
            "503": { "": "" }
        }

        """
        self.blackboard.blackboard["attack_id"] = self.scenario_name
        self.blackboard.blackboard["actions_log"] = []
        self.blackboard.blackboard["reward_log"] = []
        self.blackboard.blackboard["errors"] = []
        self.blackboard.blackboard["timestamps"]["first_packet"] = None
        self.blackboard.blackboard["timestamps"]["last_packet"] = None
        self.blackboard.blackboard["runtime_behavior"] = {
            "shell_opened": {
                "shell_type": "",
                "session_type": "",
                "shell_access_level": "",
                "authentication_method": "",
                "shell_session": { "commands_run": [] }
            }
        }
        """
        print(f"[+] Starting scenario: {self.scenario_name}")

    def should_continue(self):
        """
        ×‘×•×“×§ ×”×× ×™×© ×œ×”××©×™×š ××ª ×”×ª×¨×—×™×© ××• ×œ×”×¤×¡×™×§ ××•×ª×• ×œ×¤×™ ×ª× ××™× ×©×”×•×’×“×¨×•.
        """
        if not self.active:
            return False
        if self.current_step >= self.max_steps:
            print("[!] Max steps reached.")
            return False
        for condition in self.stop_conditions:
            if condition(self.blackboard.blackboard):
                print("[!] Stop condition met.")
                return False
        return True

    def step(self):
        """
        ××‘×¦×¢ ×¦×¢×“ ××—×“ ×©×œ ×”×¡×™××•×œ×¦×™×” ×¢"×™ ×”×¤×¢×œ×ª AgentManager.
        """
        print(f"[>] Running step {self.current_step}...")
        self.agent_manager.run_step()
        self.current_step += 1

    def end(self):
        """
        ×¡×•×’×¨ ××ª ×”×ª×¨×—×™×© ×•××¡××Ÿ ××ª ×”×¡×™××•×œ×¦×™×” ×›×œ× ×¤×¢×™×œ×”.
        """
        self.active = False
        #self.blackboard.blackboard["timestamps"]["last_packet"] = self.current_step
        print(f"[+] Scenario '{self.scenario_name}' ended after {self.current_step} steps.")

    def run_scenario_loop(self):
        """
        ××¤×¢×™×œ ××ª ×›×œ ×”×ª×¨×—×™×© ×‘×œ×•×œ××” ×¢×“ ×œ×¡×™×•×.
        """
        self.start()
        while self.should_continue():
            self.step()
        self.end()

<</mnt/linux-data/project/code/models/policy_model.py>>
import torch
import torch.nn as nn
import torch.nn.functional as F

class PolicyModel(nn.Module):
    def __init__(self, state_size, action_size, hidden_sizes=[128, 64], learning_rate=1e-3, gamma=0.99):
        super(PolicyModel, self).__init__()
        self.fc1 = nn.Linear(state_size, hidden_sizes[0])
        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])
        self.output = nn.Linear(hidden_sizes[1], action_size)

        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)
        self.loss_fn = nn.MSELoss()
        self.gamma = gamma  # discount factor

    def forward(self, state_vector):
        """
        ××§×‘×œ state_vector ×•××—×–×™×¨ Q-values ×œ×›×œ ×¤×¢×•×œ×”.
        """
        if not isinstance(state_vector, torch.Tensor):
            state_vector = torch.tensor(state_vector, dtype=torch.float32)

        if state_vector.ndim == 1:
            state_vector = state_vector.unsqueeze(0)  # ×”×•×¡×¤×ª ××™××“ Batch

        x = F.relu(self.fc1(state_vector))
        x = F.relu(self.fc2(x))
        return self.output(x)  # No softmax â€“ Q-values ×××©×™×™×

    def predict_best_action(self, state_vector):
        """
        ××—×–×™×¨ ××ª ××™× ×“×§×¡ ×”×¤×¢×•×œ×” ×¢× Q-value ×”×’×‘×•×” ×‘×™×•×ª×¨.
        """
        with torch.no_grad():
            q_values = self.forward(state_vector)
            return torch.argmax(q_values).item()

    def update(self, experience):
        """
        ××¢×“×›×Ÿ ××ª ×”×¨×©×ª ×¢×œ ×‘×¡×™×¡ × ×™×¡×™×•×Ÿ ×‘×•×“×“ (state, action, reward, next_state).
        """
        state = experience["state"].clone().detach().unsqueeze(0)
        action = torch.tensor([experience["action"]], dtype=torch.long)
        reward = torch.tensor([experience["reward"]], dtype=torch.float32)
        next_state = experience["next_state"].clone().detach().unsqueeze(0)

        # ×—×™×©×•×‘ Q(s, a)
        q_values = self.forward(state)
        q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)  # ×©×•××¨ ×¢×œ [batch]

        # ×—×™×©×•×‘ max_a' Q(next_state, a')
        next_q_values = self.forward(next_state)
        max_next_q_value = next_q_values.max(1)[0].detach()

        # TD Target
        td_target = reward + self.gamma * max_next_q_value
        td_target = td_target.view(-1)

        # Loss = MSE(Q, TD_target)
        loss = self.loss_fn(q_value, td_target)

        # ×©×œ×‘ ×”××™××•×Ÿ
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        return q_value.item(), loss.item()

    def save(self, path):
        torch.save(self.state_dict(), path)

    def load(self, path):
        self.load_state_dict(torch.load(path))
        self.eval()

<</mnt/linux-data/project/code/models/LoadModel.py>>
import subprocess
import os

class LoadModel:
    def __init__(self, llama_path, model_path, tokens=2000, threads=4):
        self.llama_path = llama_path
        self.model_path = model_path
        self.tokens = str(tokens)
        self.threads = str(threads)

        if not os.path.isfile(self.llama_path):
            raise FileNotFoundError(f"llama-run binary not found: {self.llama_path}")
        if not self.model_path.startswith("file://"):
            raise ValueError("model_path must start with 'file://'")

        print("âœ… LoadModel initialized successfully.")
    
    def run_prompt(self, prompt):
        cmd = [
            self.llama_path,
            self.model_path,
            prompt,
            "-n", self.tokens,
            "-t", self.threads
        ]
        try:
            output = subprocess.check_output(cmd, text=True)
            return output.strip()
        except subprocess.CalledProcessError as e:
            print("âŒ llama-run failed:")
            print(e.output)
            return None


    def run_prompts(self, prompts):
        responses = []
        context = ""
        for prompt in prompts:
            full_prompt = context + "\n" + prompt if context else prompt
            cmd = [
                self.llama_path,
                self.model_path,
                full_prompt,
                "-n", self.tokens,
                "-t", self.threads
            ]
            try:
                output = subprocess.check_output(cmd, text=True)
                responses.append(output)
                context += f"\n{prompt}\n{output.strip()}"
            except subprocess.CalledProcessError as e:
                print("âŒ llama-run failed:")
                print(e.output)
                responses.append(None)
        return responses

<</mnt/linux-data/project/code/models/run.py>>
from LoadModel import LoadModel
from prompts import PROMPT_FOR_A_PROMPT

# ×”×’×“×¨×ª ×”× ×ª×™×‘×™×
LLAMA_RUN = "/mnt/linux-data/project/code/models/llama.cpp/build/bin/llama-run"
MODEL_PATH = "file:///mnt/linux-data/project/code/models/nous-hermes/Nous-Hermes-2-Mistral-7B-DPO.Q4_K_M.gguf"

# ×™×¦×™×¨×ª ××•×¤×¢ ×©×œ ×”××—×œ×§×” ×•×”×¤×¢×œ×ª ×”×¨×¦×£
model = LoadModel(LLAMA_RUN, MODEL_PATH)
command_output = """
<html><head><title>Metasploitable2 - Linux</title></head><body>
<pre>

                _                  _       _ _        _     _      ____  
 _ __ ___   ___| |_ __ _ ___ _ __ | | ___ (_) |_ __ _| |__ | | ___|___ \ 
| '_ ` _ \ / _ \ __/ _` / __| '_ \| |/ _ \| | __/ _` | '_ \| |/ _ \ __) |
| | | | | |  __/ || (_| \__ \ |_) | | (_) | | || (_| | |_) | |  __// __/ 
|_| |_| |_|\___|\__\__,_|___/ .__/|_|\___/|_|\__\__,_|_.__/|_|\___|_____|
                            |_|                                          


Warning: Never expose this VM to an untrusted network!

Contact: msfdev[at]metasploit.com

Login with msfadmin/msfadmin to get started


</pre>
<ul>
<li><a href="/twiki/">TWiki</a></li>
<li><a href="/phpMyAdmin/">phpMyAdmin</a></li>
<li><a href="/mutillidae/">Mutillidae</a></li>
<li><a href="/dvwa/">DVWA</a></li>
<li><a href="/dav/">WebDAV</a></li>
</ul>
</body>
</html>
"""
response = model.run_prompt(PROMPT_FOR_A_PROMPT(command_output))

print(response)

<</mnt/linux-data/project/code/models/trainer.py>>
import torch
import torch.nn as nn
import torch.optim as optim
import random
import copy  # ×‘×ª×—×™×œ×ª ×”×§×•×‘×¥

class RLModelTrainer:
    def __init__(self, policy_model, replay_buffer, device='cpu', learning_rate=1e-3, gamma=0.99):
        """
        ××××Ÿ ××ª ××•×“×œ ×”Ö¾policy (Q-Network) ×œ×¤×™ × ×ª×•× ×™× ××”Ö¾ReplayBuffer.
        
        Parameters:
        - policy_model: ××•×“×œ ×”Ö¾neural network ×”×××¤×” state_vector â†’ q_values.
        - replay_buffer: ××•×¤×¢ ×©×œ ReplayBuffer ×”××›×™×œ × ×™×¡×™×•× ×•×ª (state, action, reward, next_state).
        - device: 'cpu' ××• 'cuda'.
        - learning_rate: ×§×¦×‘ ×”×œ××™×“×” (Î±).
        - gamma: discount factor (Î³) ×¢×‘×•×¨ ×—×™×©×•×‘ TD-target.
        """
        self.policy_model = policy_model.to(device)
        self.replay_buffer = replay_buffer
        self.device = device
        self.gamma = gamma
        self.optimizer = optim.Adam(self.policy_model.parameters(), lr=learning_rate)
        self.loss_fn = nn.MSELoss()
        self.training_history = []
        self.target_model = copy.deepcopy(policy_model).to(device)
        self.target_model.eval()
        self.update_target_steps = 100  # ×›×œ ×›××” ×¦×¢×“×™× ×œ×¢×“×›×Ÿ
        self.train_step = 0

    def train_batch(self, batch_size):
        """
        ××“×’× batch ××”Ö¾PrioritizedReplayBuffer ×•××¢×“×›×Ÿ ××ª ×”××•×“×œ ×œ×¤×™ TD-learning (DQN).
        ××—×–×™×¨ ××ª ×¢×¨×š ×”××™×‘×•×“ (loss) ×¢×‘×•×¨ ×”××™××•×Ÿ.
        """
        if self.replay_buffer.size() < batch_size:
            return None  # ×œ× ××¡×¤×™×§ × ×ª×•× ×™× ×œ××™××•×Ÿ

        # ××“×’× ×—×•×•×™×•×ª ×‘×¢×“×™×¤×•×ª (Prioritized Sampling)
        states, actions, rewards, next_states, dones, weights, indices = self.replay_buffer.sample_batch(batch_size)
        
        states = states.to(self.device)
        actions = actions.to(self.device)
        rewards = rewards.to(self.device)
        next_states = next_states.to(self.device)
        dones = dones.to(self.device)
        weights = torch.tensor(weights, dtype=torch.float32).to(self.device)

        # ×—×™×©×•×‘ Q-values ×¢×‘×•×¨ ×”Ö¾states ×”× ×•×›×—×™×™×:
        q_values = self.policy_model.forward(states)
        current_q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)

        # ×—×™×©×•×‘ Q-values ×¢×‘×•×¨ ×”Ö¾next_states:
        next_q_values = self.target_model.forward(next_states)
        max_next_q_values = next_q_values.max(1)[0]
        dones = dones.float()
        td_target = rewards + self.gamma * max_next_q_values * (1 - dones)

        td_target = td_target.detach()

        # ×—×™×©×•×‘ ×”Ö¾TD error
        td_errors = torch.abs(current_q_values - td_target)

        # ×—×™×©×•×‘ ×”××™×‘×•×“ ×¢×œ ×¤×™ ×”-weight ×©×œ ×›×œ ×—×•×•×™×”
        loss = (weights * td_errors).mean()

        # ×‘×™×¦×•×¢ backpropagation
        self.optimizer.zero_grad()
        loss.backward()

        self.train_step += 1
        if self.train_step % self.update_target_steps == 0:
            self.target_model.load_state_dict(self.policy_model.state_dict())

        self.optimizer.step()

        # ×¢×“×›×•×Ÿ ×”-priorities
        self.replay_buffer.update_priorities(indices, td_errors.detach().cpu().numpy())

        return loss.item()

    def evaluate_action(self, state):
        """
        ××§×‘×œ state (×•×§×˜×•×¨) ×•××—×–×™×¨ ××ª Q-values ×œ×›×œ ×”×¤×¢×•×œ×•×ª ×”××¤×©×¨×™×•×ª.
        """
        with torch.no_grad():
            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)
            q_values = self.policy_model.forward(state_tensor)
            return q_values.cpu().numpy()

    def save_model(self, path):
        """
        ×©×•××¨ ××ª ×”××•×“×œ ×œ×“×™×¡×§.
        """
        torch.save(self.policy_model.state_dict(), path)

    def load_model(self, path):
        """
        ×˜×•×¢×Ÿ ××ª ×”××•×“×œ ××”×“×™×¡×§.
        """
        self.policy_model.load_state_dict(torch.load(path, map_location=self.device))
        self.policy_model.eval()

    def plot_learning_curve(self):
        """
        ××¦×™×’ ×’×¨×£ ×©×œ ××™×‘×•×“ ×”××™××•×Ÿ ×œ××•×¨×š ×–××Ÿ.
        """
        import matplotlib.pyplot as plt
        plt.plot(self.training_history)
        plt.xlabel("Training Iterations")
        plt.ylabel("Loss")
        plt.title("Learning Curve")
        plt.show()

<</mnt/linux-data/project/code/models/prompts.py>>
PROMPT_1 = """ You are about to receive a specific JSON structure. You must remember it exactly as-is.

Do not explain, summarize, or transform it in any way.
Just memorize it internally â€” you will be asked to use it later.

Here is the structure:
{
  "target": {
    "ip": "",
    "os": "Unknown",
    "services": [
      {"port": "", "protocol": "", "service": ""},
      {"port": "", "protocol": "", "service": ""},
      {"port": "", "protocol": "", "service": ""}
    ]
  },
  "web_directories_status": {
    "404": { "": "" },
    "200": { "": "" },
    "403": { "": "" },
    "401": { "": "" },
    "503": { "": "" }
  },
}
"""

def PROMPT_2(command_output: str, Custom_prompt: str) -> str:
  return f"""
    You were previously given a specific JSON structure. You MUST now return ONLY that same structure, filled correctly.

    ğŸ›‘ Do NOT:
    - Change or rename any fields
    - Add new keys like "network_services", "port_state", or "service_name"
    - Nest data inside services or add arrays like "web_directories"
    - Remove or replace any part of the original structure
    - GUESS or INVENT any values that were not explicitly provided
    - Capitalize service or protocol names â€” always use lowercase only
    - Fill "ip" unless it is clearly the target machine
    - Use vague messages like "Active" â€” only real messages are allowed

    âœ… You MUST:
    - Return a JSON with the following two root-level keys only: "target" and "web_directories_status"
    - Inside "target", include: "ip" (string), "os" (string), and "services" (array of {{\"port\", \"protocol\", \"service\"}})
    - Inside "web_directories_status", keep ONLY the keys: "200", "401", "403", "404", "503" â€” each must be a dictionary mapping directory paths (e.g., "admin/") to messages
    - You MUST include ALL valid services found in the data.
    - There is NO limit on how many services can be returned.
    - Even if the example structure had 3 entries â€” return as many as needed.

    ğŸš« If no IP, OS, or web directories were provided, leave them exactly as-is:
    - "ip": ""
    - "os": "Unknown"
    - web_directories_status must include exactly these keys: "200", "401", "403", "404", "503"
    - If no web directory paths are available for a status, use: {{ "": "" }} as its value

    â— Each entry in the "services" array MUST be a JSON object with:
    - "port": number
    - "protocol": string (lowercase only)
    - "service": string (lowercase only)

    Instructions for this spesific command:

    {Custom_prompt}

    Here is the new data:

    {command_output}

    ğŸ§ª Before returning your answer:
    - Compare it to the original structure character by character
    - Return ONLY ONE JSON â€” no explanation, no formatting, no comments
  """

def clean_output_prompt(raw_output: str) -> str:
    return f"""
You are given raw output from a network-related command.

ğŸ¯ Your task is to extract only the **critical technical reconnaissance data** relevant to identifying, fingerprinting, or mapping a **target system**.

â— You do **NOT** know which command was used â€” it could be `whois`, `dig`, `nmap`, `nslookup`, `curl -I`, or any other.

âœ… KEEP only the following:
- IP ranges, CIDRs, hostnames, domain names
- Open ports and services (e.g., "80/tcp open http")
- Server headers and fingerprinting info (e.g., Apache, nginx, PHP, IIS, versions)
- WHOIS identifiers: NetName, NetHandle, NetType, CIDR
- RFC references and technical timestamps (RegDate, Updated)
- Technical metadata directly describing network blocks or infrastructure

ğŸ›‘ REMOVE everything else, including:
- Organization identity fields: `OrgName`, `OrgId`, `OrgTechName`, `OrgAbuseName`, etc.
- Geographical location: `City`, `Country`, `PostalCode`, `Address`
- Abuse or tech contact details: `OrgAbuseEmail`, `OrgTechEmail`, `Phone numbers`
- Legal disclaimers or registry policy messages (ARIN/RIPE/IANA)
- Any general comments, public notices, usage suggestions, or boilerplate text
- Duplicate fields or references (e.g., "Ref:", "Parent:")
- Empty fields, formatting headers, decorative characters

âš ï¸ DO NOT:
- Rephrase anything
- Add explanations or summaries
- Invent missing values

Return ONLY the cleaned, relevant technical output that a **penetration tester or AI agent** could use to understand the target system.

---

Here is the raw output:
---
{raw_output}
---

Return ONLY the cleaned output. No explanations, no formatting.
"""

def PROMPT_FOR_A_PROMPT(raw_output: str) -> str:
    return f"""
You are an LLM tasked with analyzing raw output from a reconnaissance command.

Your job is to generate **precise instructions** that guide another LLM on how to extract technical information from this specific output.  
ğŸ›  The instructions must describe **how to interpret the output**, **what patterns to look for**, and **what information is relevant**.

ğŸ¯ Your output must:
- Identify the type of information present (e.g., IP, OS, ports, services)
- Specify **how to locate that data** (e.g., line structure, keywords, formats)
- Describe each relevant field that should be extracted
- Be tailored **specifically to this output** â€” not generic

âŒ Do **NOT** include:
- Any example JSON structure
- Any explanations, summaries, or assumptions
- Any formatting instructions or extra commentary

ğŸ“¥ Here is the raw output to analyze:

{raw_output}

âœï¸ Return only a clear and focused list of extraction instructions based on the data in this output.
"""

<</mnt/linux-data/project/code/blackboard/blackboard.py>>
def initialize_blackboard():
    return {
        "target": {
            "ip": "",
            "os": "Unknown",
            "services": [
            {"port": "", "protocol": "", "service": ""},
            {"port": "", "protocol": "", "service": ""},
            {"port": "", "protocol": "", "service": ""}
            ]
        },
        "web_directories_status": {
            "404": { "": "" },
            "200": { "": "" },
            "403": { "": "" },
            "401": { "": "" },
            "503": { "": "" }
        },
        "actions_history": {}
    }


<</mnt/linux-data/project/code/blackboard/blackboard_all.py>>
def initialize_blackboard():
    return {
        "attack_id": "",
        "target": {
            "ip": "",
            "os": "",
            "services": []
        },
        "exploit_metadata": {
            "exploit_id": None,
            "exploit_title": "",
            "exploit_language": None,
            "exploit_type": "",
            "source": "",
            "exploit_path": None
        },
        "exploit_code_raw": None,
        "code_static_features": {
            "functions": [],
            "syscalls_used": [],
            "payload_type": "",
            "encoding_methods": [],
            "obfuscation_level": "",
            "external_dependencies": [],
            "exploit_technique": ""
        },
        "connections_summary": {
            "total_connections": None,
            "total_packets": None,
            "protocols": [],
            "ports_involved": [],
            "flags_observed": [],
            "data_transferred_bytes": None,
            "sessions": []
        },
        "payload_analysis": [],
        "runtime_behavior": {
            "shell_opened": {
                "shell_type": "",
                "session_type": "",
                "shell_access_level": "",
                "authentication_method": "",
                "shell_session": {
                    "commands_run": []
                }
            }
        },
        "timestamps": {
            "first_packet": None,
            "last_packet": None
        },
        "attack_impact": {
            "success": None,
            "access_level": "",
            "shell_opened": None,
            "shell_type": "",
            "authentication_required": None,
            "persistence_achieved": None,
            "data_exfiltrated": [],
            "sensitive_info_exposed": [],
            "log_files_modified": [],
            "detected_by_defenses": None,
            "quality_score": None
        },
        "actions_log": [],
        "errors": [],
        "reward_log": [],
        "exploits_attempted": [],
        "explanation_of_attack": "",
        "exploit_analysis_detailed": []
    }


<</mnt/linux-data/project/code/blackboard/api.py>>
import time
import copy
import json
import re
from blackboard.utils import extract_json

class BlackboardAPI:
    def __init__(self, blackboard_dict: dict):
        self.blackboard = blackboard_dict

    def get_state_for_agent(self, agent_name: str) -> dict:
        """
        ××—×–×™×¨ ××ª ×›×œ ×”Ö¾Blackboard ×›×¤×™ ×©×”×•×, ×œ×œ× ×¡×™× ×•×Ÿ ×œ×¤×™ ×¡×•×’ ×”×¡×•×›×Ÿ.
        """
        return copy.deepcopy(self.blackboard)

    def update_runtime_behavior(self, info_dict: dict):
        """
        ××¢×“×›×Ÿ ××ª ×ª×ªÖ¾×”××‘× ×” 'runtime_behavior' ×¢× ×”××™×“×¢ ×”×—×“×© ×©×”×ª×§×‘×œ ××”×¤×§×•×“×”.
        ×ª×•××š ×‘×”×•×¡×¤×ª ××¤×ª×—×•×ª ×—×“×©×™× ××• ×¢×“×›×•×Ÿ ×—×›× ×©×œ ×§×™×™××™×.
        """
        runtime = self.blackboard.setdefault("runtime_behavior", {})

        for key, value in info_dict.items():
            if isinstance(value, list):
                # ×× ×–×• ×¨×©×™××”, × ××—×“ ×¢× ×¨×©×™××” ×§×™×™××ª ×ª×•×š ×”×™×× ×¢×•×ª ××›×¤×•×œ×™×
                existing = runtime.get(key, [])
                merged = list(set(existing + value))
                runtime[key] = merged
            elif isinstance(value, dict):
                # ×× ×–×” ××‘× ×” ××§×•× ×Ÿ â€“ × ×¢×©×” ×¢×“×›×•×Ÿ ×¨×§ ×‘×¨××” ××—×ª
                existing = runtime.get(key, {})
                if not isinstance(existing, dict):
                    existing = {}
                existing.update(value)
                runtime[key] = existing
            else:
                # ×¢×¨×›×™× ×¤×©×•×˜×™× â€“ ×¤×©×•×˜ ×œ×¢×“×›×Ÿ
                runtime[key] = value

    def append_action_log(self, entry: dict):
        """
        ××•×¡×™×£ ×¨×©×•××” ×œ×œ×•×’ ×”×¤×¢×•×œ×•×ª.
        """
        entry["timestamp"] = time.time()
        self.blackboard.setdefault("actions_log", []).append(entry)

    def record_reward(self, action: str, reward: float):
        """
        ×©×•××¨ ××ª ×¢×¨×š ×”×ª×’××•×œ ×œ×¤×¢×•×œ×” ×”××—×¨×•× ×” ×©×‘×•×¦×¢×”.
        """
        entry = {
            "action": action,
            "reward": reward,
            "timestamp": time.time()
        }
        self.blackboard.setdefault("reward_log", []).append(entry)

    def add_error(self, agent: str, action: str, error: str):
        """
        ××ª×¢×“ ×©×’×™××” ×©×”×ª×¨×—×©×” ×‘××”×œ×š ×”×¨×¦×ª ×¤×¢×•×œ×”.
        """
        entry = {
            "agent": agent,
            "action": action,
            "error": error,
            "timestamp": time.time()
        }
        self.blackboard.setdefault("errors", []).append(entry)

    def get_last_actions(self, agent: str, n: int = 5):
        """
        ××—×œ×¥ ××ª N ×”×¤×¢×•×œ×•×ª ×”××—×¨×•× ×•×ª ×©×‘×™×¦×¢ ×¡×•×›×Ÿ ××¡×•×™×.
        """
        return [
            log for log in reversed(self.blackboard.get("actions_log", []))
            if log["agent"] == agent
        ][:n]

    def update_target_services(self, new_services: list):
        """
        ××•×¡×™×£ ×©×™×¨×•×ª×™× ×—×“×©×™× ×œÖ¾target.services ×× ×”× ×¢×“×™×™×Ÿ ×œ× ×§×™×™××™×.
        """
        existing = self.blackboard["target"].get("services", [])
        for service in new_services:
            if service not in existing:
                existing.append(service)

    def update_exploit_metadata(self, exploit_data: dict):
        """
        ××¢×“×›×Ÿ ××ª ×”××™×“×¢ ×¢×œ Exploit ××—×¨×•×Ÿ ×©× ×‘×—×¨.
        """
        self.blackboard["exploit_metadata"].update(exploit_data)

    def overwrite_blackboard(self, new_state: dict):
        """
        ××¢×“×›×Ÿ ××ª ×”Ö¾blackboard ×¢× new_state,
        ×•×©×•××¨ ×¨×§ ××ª ×”×©×“×” actions_history ××”××¦×‘ ×”×§×™×™×.
        """
        if not isinstance(new_state, dict):
            raise ValueError("new_state must be a dictionary")

        # ×©××™×¨×ª actions_history ×‘×œ×‘×“
        preserved_actions_history = self.blackboard.get("actions_history", {})

        # ××™×¤×•×¡ ××œ×
        self.blackboard.clear()

        # ×¢×“×›×•×Ÿ ×¢× ×”××™×“×¢ ×”×—×“×© ××”Ö¾LLM
        self.blackboard.update(new_state)

        # ×”×•×¡×¤×ª ×”×™×¡×˜×•×¨×™×™×ª ×”×¤×¢×•×œ×•×ª ×—×–×¨×”
        self.blackboard["actions_history"] = preserved_actions_history

<</mnt/linux-data/project/code/blackboard/utils.py>>
import json
import re

def extract_json(text: str):
    """
    ××—×œ×¥ ××ª ××‘× ×” ×”-JSON ×”×ª×§× ×™ ×”××—×¨×•×Ÿ ××ª×•×š ×˜×§×¡×˜ (×œ×œ× ×©×™××•×© ×‘-?R).
    """
    # × ×—×¤×© ×‘×œ×•×§×™× ×©×™×›×•×œ×™× ×œ×”×™×•×ª JSON (××”×ª×•×•×™× { ... })
    json_like_blocks = re.findall(r"{[\s\S]*?}", text)

    for block in reversed(json_like_blocks):  # × ×ª×—×™×œ ××”×¡×•×£
        try:
            return json.loads(block)
        except json.JSONDecodeError:
            continue

    raise ValueError(f"âŒ Failed to extract valid JSON from:\n{text}")

<</mnt/linux-data/project/code/utils/utils.py>>
import json
import re

def extract_json_block(text_input):
    """
    ××§×‘×œ×ª ×˜×§×¡×˜ (××• ×¨×©×™××” ×©×œ ×˜×§×¡×˜×™×), ×•××—×–×™×¨×” ××ª ×‘×œ×•×§ ×”Ö¾JSON ×”×ª×§× ×™ ×”×›×™ ×’×“×•×œ ×©××¦××” ×›Ö¾dict.
    """
    if isinstance(text_input, list):
        text = "\n".join(text_input)
    else:
        text = str(text_input)

    # × ×™×§×•×™ escape codes (×›××• \x1b[0m)
    text = re.sub(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])', '', text)

    # ×—×™×¤×•×© ×›×œ ×”×‘×œ×•×§×™× ×©××ª×—×™×œ×™× ×•× ×’××¨×™× ×‘×¡×•×’×¨×™×™× ××¡×•×œ×¡×œ×™×
    matches = []
    stack = []
    start = None

    for i, char in enumerate(text):
        if char == '{':
            if not stack:
                start = i
            stack.append('{')
        elif char == '}':
            if stack:
                stack.pop()
                if not stack and start is not None:
                    candidate = text[start:i+1]
                    matches.append(candidate)

    # × ×¡×” ×œ×¤×¨×¡×¨ ×›×œ ××—×“ ×•×œ×‘×—×•×¨ ××ª ×”×›×™ ×’×“×•×œ
    valid_jsons = []
    for candidate in matches:
        try:
            obj = json.loads(candidate)
            valid_jsons.append((candidate, obj))
        except json.JSONDecodeError:
            continue

    if not valid_jsons:
        print("âŒ No valid JSON found.")
        return None

    # ×‘×—×¨ ××ª ×”××—×¨×•×–×ª ×”×›×™ ××¨×•×›×” (×›Ö¾string)
    best_candidate = max(valid_jsons, key=lambda pair: len(pair[0]))
    return best_candidate[1]  # ××—×–×™×¨ ××ª ×”Ö¾dict


def remove_comments_and_empty_lines(text: str) -> str:
    """
    ××•×—×§×ª ××ª ×›×œ ×”×©×•×¨×•×ª ×©××ª×—×™×œ×•×ª ×‘Ö¾# ×•××ª ×›×œ ×”×©×•×¨×•×ª ×”×¨×™×§×•×ª ××ª×•×š ×”×˜×§×¡×˜.
    """
    cleaned_lines = []
    for line in text.splitlines():
        stripped = line.strip()
        if stripped and not stripped.startswith("#"):
            cleaned_lines.append(line)
    return "\n".join(cleaned_lines)


def main():
    text_input = [
        '[I will memorize the provided JSON structure exactly as-is:\n\n{\n  "target": {\n    "ip": "",\n    "os": "Unknown",\n    "services": [\n      {"port": "", "protocol": "", "service": ""},\n      {"port": "", "protocol": "", "service": ""},\n      {"port": "", "protocol": "", "service": ""}\n    ]\n  },\n  "web_directories_status": {\n    "404": { "": "" },\n    "200": { "": "" },\n    "403": { "": "" },\n    "401": { "": "" },\n    "503": { "": "" }\n  }\n}\x1b[0m\n',
        '{\n  "target": {\n    "ip": "",\n    "os": "Unknown",\n    "services": [\n      {"port": "", "protocol": "", "service": ""},\n      {"port": "", "protocol": "", "service": ""},\n      {"port": "", "protocol": "", "service": ""}\n    ]\n  },\n  "web_directories_status": {\n    "404": { "": "" },\n    "200": { "": "" },\n    "403": { "": "" },\n    "401": { "": "" },\n    "503": { "": "" }\n  }\n}\x1b[0m\n]'
    ]

    result = extract_json_block(text_input)
    print("âœ… Extracted JSON:")
    print(json.dumps(result, indent=2))


if __name__ == "__main__":
    main()
<</mnt/linux-data/project/code/encoders/state_encoder.py>>
import hashlib
import json
import torch
import numbers
import numpy as np

class StateEncoder:
    def __init__(self, action_space: list, max_features: int = 128):
        self.encoded_to_state = {}  # mapping hash â†’ original state
        self.max_features = max_features  # ×’×•×“×œ ×§×‘×•×¢ ×œ×•×•×§×˜×•×¨ ×”×§×œ×˜
        self.action_space = action_space  # ×›×œ ×”×¤×§×•×“×•×ª ×”××¤×©×¨×™×•×ª
        self.action_to_index = {action: i for i, action in enumerate(action_space)}

    def base100_encode(self, text: str) -> float:
        """
        ××‘×¦×¢ ×§×™×“×•×“ ××—×¨×•×–×ª ×œ×¢×¨×š ×¢×©×¨×•× ×™ ×‘×˜×•×•×— [0, 1) ×œ×¤×™ ×‘×¡×™×¡ 100.
        """
        base = 100
        code = 0
        for i, c in enumerate(text[:5]):
            code += ord(c) * (base ** (4 - i))
        max_code = (base ** 5) - 1
        return code / max_code

    def encode(self, state: dict, actions_history: list) -> torch.Tensor:
        state_str = json.dumps(state, sort_keys=True, separators=(',', ':'))
        state_hash = hashlib.sha256(state_str.encode()).hexdigest()

        self.encoded_to_state[state_hash] = state

        flat_state = self._flatten_state(state)

        actions_vector = np.zeros(len(self.action_space), dtype=np.float32)
        for action in actions_history:
            if action in self.action_to_index:
                idx = self.action_to_index[action]
                actions_vector[idx] = 1.0

        for i, val in enumerate(actions_vector):
            flat_state[f"action_history_idx_{i}"] = val

        sorted_items = sorted(flat_state.items())
        encoded_values = [self._normalize_value(k, v) for k, v in sorted_items]

        if len(encoded_values) < self.max_features:
            encoded_values += [0.0] * (self.max_features - len(encoded_values))
        else:
            encoded_values = encoded_values[:self.max_features]

        vector = torch.tensor(encoded_values, dtype=torch.float32)
        print(f"[Encoder] Encoded vector of length {len(encoded_values)} (state + history)")
        return vector

    def decode(self, state_hash: str) -> dict:
        return self.encoded_to_state.get(state_hash, {})

    def _flatten_state(self, obj, prefix='') -> dict:
        """
        ×”×•×¤×š ××‘× ×” ××§×•× ×Ÿ (dict/list) ×œ××™×œ×•×Ÿ ×©×˜×•×— ×©×œ feature_name â†’ numeric_value.
        ×›×•×œ×œ ×©×™××•×© ×‘×§×™×“×•×“ base100 ×¢×‘×•×¨ ××—×¨×•×–×•×ª.
        """
        items = {}
        if isinstance(obj, dict):
            for k, v in obj.items():
                full_key = f"{prefix}.{k}" if prefix else k
                items.update(self._flatten_state(v, full_key))
        elif isinstance(obj, list):
            for i, v in enumerate(obj):
                full_key = f"{prefix}[{i}]"
                items.update(self._flatten_state(v, full_key))
        elif isinstance(obj, bool):
            items[prefix] = 1.0 if obj else 0.0
        elif isinstance(obj, numbers.Number):
            items[prefix] = float(obj)
        elif isinstance(obj, str):
            items[prefix] = self.base100_encode(obj)
        else:
            items[prefix] = 0.0
        return items

    def _normalize_value(self, key: str, value: float) -> float:
        """
        × ×•×¨××œ×™×–×¦×™×” ×©×œ ×›×œ ×¢×¨×š ×œ××§×˜×¢ [0, 1] ×œ×¤×™ ×¡×•×’ ×”××™×“×¢ ×‘××¤×ª×—.
        """
        if isinstance(value, (int, float)):
            if "port" in key:
                return min(value / 65535.0, 1.0)
            elif "protocol" in key:
                return min(value / 3.0, 1.0)
            elif "action_history" in key:
                return float(value)
            elif "service" in key:
                return min(value / 1000000.0, 1.0)
            elif "web_directories_status" in key:
                return min(value / 1000000.0, 1.0)
            elif "os" in key:
                return min(value / 1000000.0, 1.0)
            else:
                return min(value / 1000000.0, 1.0)
        else:
            return 0.0

<</mnt/linux-data/project/code/encoders/action_encoder.py>>
# action_encoder.py

from typing import List

class ActionEncoder:
    def __init__(self, actions: List[str]):
        self.action_to_index = {action: i for i, action in enumerate(actions)}
        self.index_to_action = {i: action for i, action in enumerate(actions)}

    def encode(self, action: str) -> int:
        """
        ××§×‘×œ ××—×¨×•×–×ª ×©×œ ×¤×§×•×“×” ×•××—×–×™×¨ ××ª ×”××™× ×“×§×¡ ×©×œ×”.
        """
        return self.action_to_index[action]

    def decode(self, index: int) -> str:
        """
        ××§×‘×œ ××™× ×“×§×¡ ×•××—×–×™×¨ ××ª ×”×¤×§×•×“×” ×”××ª××™××”.
        """
        return self.index_to_action[index]

